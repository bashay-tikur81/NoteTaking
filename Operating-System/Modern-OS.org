#+TITLE: Modern Operating System
#+BOOK: Tanenbaum A, Bos H 5th Edition
#+AUTHOR: Ertale81

* Introduction
** What is an OS?
*** General Overview
- Smartphones and tablets (like the Apple iPad) are just computers in a smaller package with a touch screen. They all have operating systems. Android smartphones and tablets all run Linux as OS on the bare hardware. Android is simply a layer of software running on top of Linux.
- The program that users interact with, usually called *shell* when it is text based and the *GUI(Graphical User Interface)* when it uses icons, is actually not part of the operating system, although it uses OS to get its work done.
- Most computers have two modes of operation:
  1) *Kernel mode(supervisor mode)*- OS runs in this mode for at least some of its functionality. In this mode, it has complete access to all the hardware and can execute any instruction the machine is capable of executing.
  2) *User mode*- The rest of software runs in user mode, in which only a subset of the machine instruction is available. In particular, those instructions that affect control of the machine, determine the security boundaries, or do *I/O* are forbidden to user-mode programs.
- The user interface program, shell or GUI, is the lowest level of user mode software, and allows the user to start other programs, such as web browser, or music player ... These programs make heavy use of the operating system.
- The placement of OS is above hardware. It runs on the bare hardware and provides the base for all the other software.
*** OS as an Extended Machine
- Operating systems contain many drivers for controlling I/O devices.
- The job of OS is to create good abstractions and then implement and manage the abstract objects thus created.
- One of the major tasks of OS is to hide the (complicated, difficult, awkward...) hardware and present programs with nice, clean, elegant, consistent, abstractions to work with. OS turns the awful in to beautiful.
- OS's real customers are application programs.
*** OS as Resource Manager
- The concept of an OS as primarily providing abstractions to application programs is a top-down view. An alternative, bottom-up, view holds that the OS is there to manage all the pieces of the complex system.
- In the bottom-up view, the job of the OS is to provide for an orderly and controlled allocation of the processors, memories, and I/O devices among the various programs wanting them.
- Resource management includes *multiplexing(sharing)* resources in two different ways:
  1) *Time multiplexing-* When a resource is time multiplexed, different programs or users take turns using it. First one of them gets to use the resource, then the another, and so on. For example: with only one CPU and multiple programs that want to run on it, the OS first allocates the CPU to one program, then, after it has run long enough, another program gets to use the CPU, then another, and eventually the first one again. Determining who gets next and for how long is the task of OS. Another example of time multiplexing is sharing the printer.
  2) *Space multiplexing*- In this resource sharing, each customer gets part of the resource, instead of taking turns. For example,  main memory is normally divided up among several running programs, each one can be resident at the same time(for example, in order to take turns using the CPU). Other resources that are space multiplexed are disks and Flash drives.
** History Of OS
*** 1st Generation(1945-1955): Vacuum Tubes
- OS have historically been closely tied to the architecture of the computers on which they run.
- All programming was done in an absolute machine language, or even worst yet, by wiring up electrical circuits by connecting thousands of cables to plugboards to control the machines basic functions.
- Programming language was unknown(even assembly language was unknown). OS was unheard of.
- The usual mode of operation was for the programmer to sign up for a block of time using the sign up sheet on the wall, then come down to the machine room, insert his/her plugboard into the computer, and spend the next few hours hoping that none of the 2,000 or so vacuum tubes would burn out during the run. Virtually all the problems were simple mathematical and numerical calculations, such as grinding out tables of sines, cosines, and logarithms, or computing artillery trajectories.
- By the early 1950, the routine had improved somewhat with the introduction punched cards. It was now possible to write programs on cards and read them instead of using plugboards.
*** 2nd Generation(1955-1965): Transistors and Batch Systems
- For the first time, there was a clear separation between designers, builders, operators, programmers, and maintenance personnel.
- To run a *job* (i.e., a program or set of programs), a programmer would first write the program on paper(in FORTRAN or assembler), then punch it on cards. The programmer would then bring the card deck down to the input room, hand it to one of the operators, and go drink coffee until the output was ready.
- When the computer finished whatever job it was currently running, an operator
  would go over to the printer and tear off the output and carry it over to the output
  room, so that the programmer could collect it later. Then the operator would take one of the card decks that had been brought from the input room and read it in. If the FORTRAN compiler was needed, the operator would have to get it from a file cabinet and read it in. Much computer time was wasted while operators were walking around the machine room.
- The solution generally adopted was the *batch system*. The idea behind it was to collect a tray full of jobs in the input room and then read them onto a magnetic tape using a (inexpensive) small computers like IBM 1401, which was good at reading cards, copying tapes, and printing output, but not quite good at numerical calculations. Other, much more expensive machines, such as IBM 7094, were used for real computing.
- After about an hour of collecting a batch of jobs, the cards were read onto a magnetic tape, which was carried into the machine room.

- Batch operating system group's jobs that perform similar types of functions. These groups are called as batch and are executed at the same time. In early 1950's batch operating system got into action at that time computer was not that developed, they had less processing power and a very minimal memory. So, at that time only one job would get executed at a time. We had to wait until the job gets executed to run the next job. We are saying execution of a job not execution of a program. So what is a job?
  + The operating system/computer at that time was not interactive. What do we mean by interactive? Let's see with an ATM. If you go to an ATM, the ATM machine will interact with you by first asking your PIN. After you enter the PIN, it will ask you for amount and other services. Basically it will take information, do the processing, ask you next question or input and do processing on it. But the computers in early stages weren't interactive. They would require everything all at once.
  + So as we need to input all information at once it was given input as a job. A job would basically consist of programs, input data, and control instructions. So, how would you give this job as input to the operating system? The input device at that time used was punch card readers and punch cards were storage device. Punch cards were stiff papers which could store digital data represented by presence or absence of holes in predefined positions. Punch cards were punched by hand or machine.
  + After preparing the job, users would give it the operator. Operator was a person who would operate the computer. He would collect the jobs from the user. There was no direct interaction of user and operating system or computer. The processing and memory power of the computer back then were not so good. It was tough to execute different types of jobs each time.
  + Let's say we have three jobs. Two of them are same(job 1 and job 3) and one(job 2) is different. And you executed in job 1 \to job 2 \to job 3 sequence. The system would then reload all the resources for job 1, take input, do process and give an output. Then it will take the job 2. As the this job is different it has to deallocate the resources from previous job and allocate the resources for new job and the whole process will work. Then job 3 will be executed, again since it is difference from the previous job, deallocation of resources and allocation of resources for this job will be done.
  + Due to less memory and processing power, execution time taken was to much and deallocation and allocation of resources for each new job would take much time. *Batch processing* was the solution for this problem. The similar kinds of job were combined in batches. Using this technique system would allocate resources for a particular type of jobs which are having similar requirement will execute one by one. This will save time for making system ready for different job each time.
  + Then the operator would make collection of jobs having similar requirements in to batches and the batch was given input to the system where once the loading of resources was done. And batch was executed and the output was generated.
*** 3rd Generation (1965-1980): ICs and Multiprogramming
- By the early 1960s, most computer manufactures had two distinct incompatible product lines. On the one hand, there were the word-oriented, large-scale scientific computers, such as 7094, which were used for industrial-strength numerical calculations in science and engineering. On the other hand, there were the character-oriented, commercial computers such as 1401, which were widely used for tape sorting and printing by banks and insurance companies.
- IBM attempted to solve both these problems at a single stroke by introducing the System/360.
- The IBM 360 was the first major computer line to use IC(Integrated Circuit), thus providing a major price/performance advantage over the second generation machines, which were built up from individual transistors.
- They popularized techniques absent on second generation OS. The most important of these was *multiprogramming*. On the 7094, when the current job paused to wait for a tape or other I/O operations to complete, the CPU simply sat idle until the I/O finished. With heavily CPU-bound scientific calculations, I/O is infrequent, so this wasted time is not significant. With commercial data processing, the I/O wait time can often be 80% or 90% of the total time, so something has to be done to avoid having the(expensive) CPU be idle so much. The solution that evolved was to partition memory into several  pieces, with a different job in each partition. While one job was waiting for I/O to complete, another job could be using the CPU. If enough jobs could be held in main memory at once, the CPU could be kept busy nearly 100% of the time. Having multiple jobs safely in memory at once requires special hardware to protect each job against snooping and mischief by other ones, but the 360 and other third-generation systems were equipped with this hardware.
- Another major feature present in third-generation OS was the ability to read jobs from cards onto the disk as soon they were brought to the computer room. Then, whenever a running job finished, the operating system could load a new job from the disk into the now-empty partition and run it. This ability is called *spooling* (from *Simultaneous peripheral Operation on Line*) and was also used for output. With spooling, the 1401s were no longer needed, and much carrying of tapes disappeared.
- Third generation computers were basically batch systems.
- Another major development during the third generation was the phenomenal growth of minicomputers, starting with DEC PDP-1 in 1961. The PDP-1 had only 4K of 18-bit words, but at $120,000 per machine( less than 5% of the price of 7094). It was quickly followed by a series of other PDPs culminating(culminate- reach climactic stage) in the PDP-11.
- One of the computer scientists at Bell Labs who had worked on the MULTICS project, Ken Thompson, subsequently found a small PDP-7 minicomputer that no one was using and set out to write a stripped-down, one-user version of MULTICS. This work later developed into the *UNIX* operating system.
- Because the source code of UNIX was widely available, various organizations developed their own(incompatible versions), which lead to chaos. Two major versions developed, *System V*, from AT&T, and *BSD(Berkeley Software Distribution)* from the university of California at Berkeley. To make it possible to write program that could run on any UNIX system, IEEE developed a standard for UNIX, called *POSIX(Portable Operating System Interface)*, that most versions of UNIX now support. POSIX defines a minimal system-call interface that conformant(conformant- conforming to a particular specification or standard) UNIX system must support.
*** 4the Generation(1980-present): Personal Computers
- With the development of *LSI(Large Scale Integration)* circuits -- chips containing thousands of transistors on a square centimeter of silicon -- the age of personal computer dawned. In terms of architecture, personal computers(initially called *minicomputers*) were not all that different from minicomputers of the PDP-11 class, but in terms of price they certainly was different.
- An interesting development that began during the mid-1980s was the development of network operating systems and distributed operating systems to manage a collection of computers.
- In a network operating system, the users are aware of the existence of multiple computers and can log in to remote machines and copy files from one machine to another. Each machine runs its own local operating system and has its own local user (or users). Such systems are not fundamentally different from single-processor operating systems. They obviously need a network interface and some low-level software to drive it, as well as programs to achieve remote login and remote file access, but these additions do not change the essential structure of the operating system.
- A distributed OS, in contrast, is one that appears to its users a traditional uniprocessor system, even though it is actually composed of multiple processors. The users should not be aware of where their programs are being run or where their files are located; that should all be handled automatically and efficiently by the operating  system.
- True distributed operating systems require more than just adding a little code to a uniprocessor operating system, because distributed and centralized systems differ in certain critical ways. Distributed systems, for example, often allow applications to run on several processors at the same time, thus requiring more complex processor scheduling algorithms in order to optimize the amount of parallelism. Moreover, communication delays within the network often mean that these (and other) algorithms must run with incomplete, outdated, or even incorrect information. This situation differs radically from that in a single-processor system in which the operating system has complete information about the system state.
*** 5th Generation(1990-present): Mobile Computers
- The first real mobile phone was appeared in 1946 and weighted 40kilos. You could take it wherever you want as long as you had a car in which to carry it.
- The first true handheld phone appeared in the 1970s and, at roughly one kilo gram.
- While the idea of combining telephony and computing in a phone-like device has been around since the 1970s also, the first real smartphone did not appear until the mid-1990s when Nokia released the N9000, which literally combined two, mostly separate devices: a phone and a Personal Digital Assistant.
- After all, most smartphones in the first decade after their inception were running *Symbian OS.* It was the operating system of choice for popular brands like Samsung, Sony Ericsson, Motorola, and especially Nokia. However, other operating systems like RIM’s Blackberry OS(introduced for smartphone in 2002) and Apple's iOS(released for the first *iPhone* in 2007) started eating into Symbian's market share. Many expected that RIM would dominate the business market, while iOS would dominate on consumer devices. Symbian’s market share plummeted. In 2011, Nokia ditched Symbian and announced it would focus on Windows Phone as its primary platform.
- But it did not take very long for Android, a Linux-based operating system released by Google in 2008, to overtake all its rivals.
- For phone manufacturers, Android had the advantage that it was open source and available under a permissive license. As a result, they could tinker with it and adapt it to their own hardware with ease. Also, it has a huge community of developers writing apps, mostly in the familiar Java programming language.
** Computer Hardware Review
*** General view
- An operating system is intimately tied to the hardware of the computer it runs on. It extends the computer's instruction set and manages its resource.
- A simple personal computer can be abstracted to a model having CPU, memory, Video controller, Keyboard controller, USB controller, and Hard disk controller. These devices are connected by a system bus and communicate with one another over it.
*** Processor
- It fetches instruction from memory and executes them. The basic cycle of every CPU is to fetch the first instruction from memory, decode it to determine its type and operands, executes it and then fetch, decode, and execute subsequent instructions. The cycle is repeated until the program finishes. In this way, programs are carried out.
- Each CPU has a specific set of instructions that it can execute. Thus an x86 processor can not execute ARM programs and an ARM(Advanced RISK Machine) processor can not execute x86 programs.
- Because accessing memory to get an instruction or data word takes much longer than executing an instruction, all CPUs contain /registers/ inside to hold key variables and temporary results.
- Instruction sets often contains instructions to load a word from memory into a register, and store a word from a register into memory. Other instructions combine two operands from registers and/or memory, into a result, such as adding two words and storing the result in a register or memory.
- In addition to the general registers used to hold variables and temporary results, most computers have several special registers that are visible to programmer. One of these is the *program counter*, which contain the memory address of the next instruction to be fetched. After that instruction has been fetched, the program counter is updated to point to its successor.
- Another register is the *stack register*, which points to the top of the current stack in memory. The stack contains one frame for each procedure that has been entered but not yet exited. A procedure's stack frame holds those input parameters, local variables, and temporary variables that are not kept in registers.
- Another register is *PSW(Program Status Word)*. This register contains the condition code bits, which are set by comparison instructions, the CPU priority, the mode(user or kernel), and various other control bits. User programs may normally read the entire PSW but typically may write only some of its fields. The PSW plays an important role in system calls and I/O.
- The OS must be fully aware of all the registers. When time multiplexing the CPU, the operating system will often stop the running program to (re)start another one. Every time it stops a running program, the operating system must save all the registers so they can be restored when the program runs later.
- In fact, we distinguish between the *architecture* and the *micro-architecture*. The architecture consists of everything that is visible to the software such as the instructions and the registers. The micro-architecture comprises the implementation of the architecture. Here we find data and instruction caches, translation lookaside buffers, branch predictors, the pipelined datapath, and many other elements that should not normally be visible to the operating system or any other software.
- To improve performance, CPU designers have long abandoned the simple model of fetching, decoding, and executing one instruction at a time. Many modern CPUs have facilities for executing more than one instruction at the same time. For example, a CPU might have separate fetch, decode, and execute units, so that while it is executing instruction n, it could also be decoding instruction n + 1 and fetching instruction n + 2. Such an organization is called a *pipeline*.
- Most CPUs, except very simple ones used in embedded systems, have (at least) two modes, kernel mode, and user mode. Usually, a bit in PSW controls the mode. When running in kernel mode, the CPU can execute every instruction in its instruction set and use every feature of the hardware. On desktop, notebook, and server machines OS runs in kernel mode, giving it access to the complete hardware. On most embedded systems, a small piece runs in kernel mode, with the rest of the OS running in user mode.
- User programs always run in user mode, which permits only a subset of the instructions to be executed and a subset of the features to be accessed. Generally, all instructions involving I/O and memory protection are disallowed in user mode. Setting the PSW mode bit to enter kernel mode is also forbidden, of course.
- To obtain services from the OS, a user program must make a *system call*, which traps into the kernel and invokes the OS. The /trap/ instruction (e.g., /syscall/ on x86) switches from user mode to kernel mode and starts the operating system.
**** Multithreaded and Multicore Chips
- Moore's law states that the number of transistors on a chip doubles every 18 months. Moore's law has held for half a century already and is expected to hold for at least a few years more. After that, the number of atoms per transistor will become too small and quantum mechanics will start to play a big role, preventing further shrinkage of transistor sizes.
- The Intel Pentium 4 introduced a property called *multithreading* or *hyperthreading* (Intel's name for it), to the x86 processor, and several other CPU chips also have it-- including the SPARC, the power5, and some ARM processors. To a first approximation, what it does is allow the CPU to hold the state of two different threads and then switch back and forth on a nanosecond time scale.(A thread is a light-weight process, which in turn is, a running program). For example, if one of the processes needs to read a word from memory(which takes many clock cycles), a multithreaded CPU can just switch to another thread. Only one process at a time is running, but thread-switching time is reduced to the order of nanoseconds.
- Multithreading has implications for the OS because each thread appears to the OS as a separate CPU.
- Beyond multithreading, many CPU chips now have four, eight, or more complete processors or *cores* on them.
*** Memory
- The second major component in any computer is memory. Ideally, memory should be extremely fast(faster than executing an instruction so that the CPU is not held up by memory), abundantly large, and dirty chip. No current technology satisfies all those goals, so a different approach is taken.
- The memory system is constructed as a hierarchy of layers. (see the figure by typing C-c C-x C-v)  [[./Memory-Hierarchy.jpeg]]
  The top layers have higher speed, smaller capacity, and greater cost per bit than the lower ones, often by a factor of a billion or more.
- The top layers consists of the registers internal to the CPU. They are made of the same material as the CPU and are as fast as the CPU. Consequently, there is no delay in accessing them. The storage capacity available in them is on the order of 32 x 32 bits on a 32-bit CPU and 64 x 64 bits on 64-bit CPU. Less than 1KB in both cases. Programs must manage the registers(i.e., decide what to keep in them) themselves, in software.
- Next comes the cache memory, which is mostly controlled by the hardware. Main memory is divided up into *cache lines*, typically 64 bytes, with address 0 to 63 in cache line 0, 64 to 127 in cache line 1, and so on. The most heavily used cache lines are kept in a high-speed cache located inside or very close to CPU. When a programs needs to read a memory word, the cache hardware checks to see if the line needed is in the cache. If it is, called a *cache hit*, the request is satisfied from the cache and no memory request is sent over the bus to the main memory.
- Cache hits normally takes a few clock cycle. Cache misses have to go to memory, with a substantial time penalty of tens to hundreds of cycles. Cache memory is limited in size due to its high cost. Some machines have two or even three levels of cache, each one slower and bigger than the one before it.
- Caching plays a major role in many areas of computer science, not just caching lines of RAM.
  Uses of cache:
  + to avoid repeated lookups(like converting long path name into a disk address)
  + resolving conversion of web URL to its IP address(not to happen every time)
- In any caching system, several questions come up fairly soon, including:
  1) When to put a new item into the cache.
  2) Which cache line to put the new item in.
  3) Which item to remove from the cache when the slot is needed
  4) Where to put a newly evicted(evicted -- eject) item in the larger memory.
- Caches are such a good idea that modern CPUs have two or more of them. The first level or *L1 cache* is always inside the CPU and usually feeds decoded instructions into the CPU’s execution engine. Most chips have a second L1 cache for very heavily used data words. The L1 caches are typically 32 KB each. In addition, there is often a second cache, called the *L2 cache*, that holds several megabytes of recently used memory words. The difference between the *L1 and L2 caches lies in the timing*. Access to the L1 cache is done without any delay, whereas access to the L2 cache involves a delay of several clock cycles.
- On multicore chips, the designer have to decide where to put the caches. For example, a single *L2 cache* can be shared by all the cores or each core can have its own *L2 cache*. Each strategy has its pros and cons. For example, the shared L2 caches requires a more complicated cache controller but the per-core L2 caches makes keeping the caches consistent more difficult.
- Main memory comes next in the hierarchy(the above image). Main memory is usually called *RAM*.(old-timers some times call it *core memory*).
- All CPU requests that can not be satisfied out of the cache go to the main memory.
- In addition to main memory, many computers have different kind of nonvolatile random-access memory. Unlike RAM, nonvolatile memory does not lose its contents when the power is switched off. *ROM (Read Only Memory)* is programmed at the factory and cannot be changed afterward. On some computers, the bootstrap loader used to start the computer is contained in ROM. *EEPROM (Electrically Erasable PROM)* is also nonvolatile, but in contrast to ROM can be erased and rewritten. However, writing it takes orders of magnitude more time than writing RAM, so it is used in the same way ROM is, except that it is now possible to correct bugs in programs by rewriting them in the field.
- Bootstrapping code may also be stored in *Flash memory*, which is similarly non volatile, but in contrast to ROM can be erased and rewritten. The bootstrapping code is commonly referred to as *BIOS(Basic Input/Output System)*.
- Flash memory is also commonly used as the storage medium in portable electronic devices such as smartphones and in SSDs to serve as a faster alternative to hard disks. Flash memory is intermediate in speed between RAM and disk. Also, unlike disk memory, if it is erased too many times, it wears out. Firmware inside the device tries to mitigate this through load balancing.
- Yet another kind of memory is *CMOS*, which is volatile. Many computers use CMOS memory to hold the current time and date. The CMOS memory and the clock circuit that increment the time in it are powered by a small battery, so the time is correctly updated, even when the computer is unplugged. The CMOS memory can also hold the configuration parameters, such as which drive to boot from. CMOS is used because it draws so little power that the original factory-installed battery often lasts for several years. However, when it begins to fail, the computer can appear to be losing its marbles, forgetting things that it has known for years, like how to boot.
- Incidentally, many computers today support a scheme known as *virtual memory*. It makes it possible to run programs larger than physical memory by placing them on non volatile storage(SSD or disk) and using the main memory as a kind of cache for the most heavily executed parts.
- In multiprogramming switching from one program to another is sometimes called a *context switch*.
*** Nonvolatile Storage
- Next in hierarchy are magnetic disks(hard disks), solid state drivers(SSDs), and persistent memory.
- A disk consists of one or more metal platters that rotate at 5400, 7200, 10,800, 15,000 RPM or more.
- Many people refer to SSDs as disks, even though they are physically not disks at all and don't have platters or moving arms. They store data in electronic(Flash) memory. The only way in which they resemble disks is in terms of hardware is that they also store a lot of data which is not lost when the power is off. But from the OS's point of view, they are somewhat like disks.
- The youngest and fastest member of the stable storage family is known as *persistent memory*. The best known example is Intel Optane which became available in 2016. In many ways, persistent memory can be seen as an additional layer between SSDs(or hard disks) and memory: it's both fast, only slightly slower than regular RAM, and it holds its content across power cycles. While it can be used to implement really fast SSDs, manufacturers may also attach it directly to the memory bus. In fact, it can be used like normal memory to store an application’s data structures, except that the data will still be there when the power goes off.
*** I/O Devices
- Besides disks there are many other I/O devices that interact heavily with the OS. I/O devices generally consist of two parts: a controller and a device itself. The controller is a chip( or a set of chips) that physically controls the device. It accepts commands from the operating system, for example, to read data from the device, and carries them out. In many cases, the actual control of the device is complicated and detailed, so it is the job of the controller to present a simpler(but still very complex) interface to the operating system.
- The other piece is the actual device itself. Devices have fairly simple interfaces, both because *they can not do much and to make them standard*. The later is needed so that any SATA disk controller can handle any SATA disk.
- *SATA(Serial ATA, ATA stands for Advanced Technology Attachment)* is currently the standard type of hard disk on many computers. Since the actual device interface is hidden behind the controller, all that the operating system sees is the interface to the controller, which may be quite different from the interface from the device.
- Because each type of controller is different, different software is needed to control each one. The software that talks to the controller, giving it commands and accepting responses, is called a *device driver*. Each controller manufacturer has to supply a driver for each OS it supports.
- To be used, the driver has to be put in to the operating system so it can run in kernel mode. Drivers can actually run outside the kernel, and operating systems like Linux and Windows nowadays do offer some support for doing so, but the vast majority of the drivers still run below the kernel boundary.
- There are three ways the driver can be put into the kernel.
  1) relink the kernel with the new driver and then reboot the system. Many older UNIX systems work this way
  2) make an entry in an OS file telling it that it needs the driver and then reboot the system. At boot time, the OS goes and finds the driver it needs and loads them. Older versions of Windows works this ways
  3) Enabling the OS to accept new drivers while running and install them on the fly without the need to reboot. Hot pluggable devices, such as USB and Thunderbolt devices are always need dynamically loaded drivers.
- Every controller has a small number of registers that are used to communicate with it. For example, a minimal disk controller might have registers for specifying the disk address, memory address, sector count, and direction (read or write). To activate the controller, the driver gets a command from the operating system, then translates it into the appropriate values to write into the device registers.
- On some computers, the device registers are mapped into the operating system's address space(the address it can use), so they can be read and written like ordinary memory words. On such computers, no special I/O instructions are required and user programs can be kept away from the hardware by not putting those memory addresses within their reach(e.g., by using base and limit registers)
- On other computers are put in a special I/O port space, which each register having a port address. On these machines, special /IN/ and /OUT/ instructions are available in kernel mode to allow drivers to read and write the registers. The former schema eliminates the need for special I/O instructions but uses up some of the address space. The latter uses no address space but requires special instructions. Both systems are widely uses.
- Input and output can be done in three different ways.
  1) The simplest method, in which a user program issues a system call, which the kernel then translates into a procedure call to the appropriate driver. The driver then starts the I/O and sits in a tight loop continuously polling the device to see if it is done(usually there is some bit that indicates that the device is still busy). When the I/O has completed, the driver puts the data(if any) where they are needed and returns. The OS then returns control to the caller. This method is called *busy waiting* and has the disadvantage of tying up the CPU polling the device until it is finished.
  2) This is for the driver to start the device and ask it to give an interrupt when it is finished. At that point the driver returns. The OS then blocks the caller if need be and looks for other work to do. When the controller detects the end of the transfer, it generates an *interrupt* to signal completion.
  3) The third method for doing I/O makes use of special hardware: a *DMA(Direct Memory Access)* chip that can control the flow of bits between memory and some controller without constant CPU intervention.The CPU sets up the DMA chip, telling it how many bytes to transfer, the device and memory addresses involved, and the direction, and lets it go. When the DMA chip is done, it causes an interrupt, the interrupt gets then handled.
*** Buses
- A system x86 has many buses(e.g., cache, memory, PCIe, PCI, USB, SATA and DMI), each with different transfer rate and function. The operating system must be aware of all of them for configuration and management.
- The main bus is the *PCIe(Peripheral Component Interconnect Express)* bus.
- The PCIe was invented by Intel as a successor of the older *PCI* bus, which in turn was a replacement for the original *ISA(Industry Standard Architecture)* bus.
- Capable of transferring tens of gigabits per second, PCIe is much faster than its predecessors. It is also very different in nature. Up to its creation in 2004, most buses were parallel and shared. A *shared bus architecture* means that multiple devices uses the same wires to transfer data. Thus, when multiple devices have data to send, you need an arbiter to determine who can use the bus. A *parallel bus architecture* as used in traditional PCI means that you send each word of data over multiple wires. For instance, in regular PCI bus, a single 32-bit number is sent over 32 parallel wires.
  In contrast this, PCIe uses a *serial bus architecture* and sends all bits in a message through a single connection, known as a *lame*, much like a network  packet.
- 
- The *USB(Universal Serial Bus)* was invented to attach all the slow I/O devices, such as the keyboard and mouse, to the computer.
- However, calling a modern USB4 device humming along at 40 Gbps ‘‘slow’’ may not come naturally for the generation that grew up with 8-Mbps ISA as the main bus in the first IBM PCs. USB uses a small connector with 4–11 wires (depending on the version), some of which supply electrical power to the USB devices or connect to ground. USB is a centralized bus in which a root device polls all the I/O devices every 1 msec to see if they have any traffic. USB 1.0 could handle an aggregate load of 12 Mbps, USB 2.0 increased the speed to 480 Mbps, USB 3.0 to 5 Gbps, USB 3.2 to 20 Gbps and USB 4 will double that. Any USB device can be connected to a computer and it will function immediately, without requiring a reboot, something pre-USB devices required, much to the consternation of a generation of frustrated users.
** The OS Zoo
*** Mainframe OS
- The OS for mainframes are heavily oriented toward processing many jobs at once, most of which need prodigious amount of I/O. They typically offer three kinds of services: *batch, transaction processing, and timesharing*.
- A batch system is one that processes routine jobs without any interactive user present. Claims processing in an insurance company or sales reporting for a chain of stores is typically done in batch mode.
- Transaction-processing systems handle large number of small requests, for example, check processing at a bank or airline reservations. Each unit of work is small, but the system must handle hundreds or thousands per second.
- Timesharing systems allow multiple remote users to run jobs on the computer at once, such as querying a big database. These functions are closely related; mainframe OS often perform all of them. An example mainframe operating system is Z/OS, the successor of OS/390, which in turn was a direct descendant of OS/360. However, mainframe operating systems are gradually being replaced by UNIX variants such as Linux.
*** Server OS
- One level down are the server OS. They run on servers, which are either very large personal computers, workstations, or even mainframes. They serve multiple users at once over a network and allow the users to share hardware and software resources. Servers can provide print service, file service, database service, or Web service.
- Typical server operating systems are Linux, FreeBSD, Solaris, and the Windows Server family.
*** Personal Computer OS
- The next category is the personal computer operating system. Modern ones all support multiprogramming, often with dozens of programs started up at boot time, and multiprocessor architecture. Their job is to provide good support to a single user. They are widely used for word processing, spreadsheets, games and Internet access. Common examples are Windows 11, macOS, Linux and FreeBSD.
*** Smartphone and Handheld Computer OS
- Continuing on down to smaller and smaller systems, we come to tablets(like Apple's iPad), smartphones and other handheld computers. A handheld computer, originally known as a *PDA (Personal Digital Assistant)*, is a small computer that can be held in your hand during operation. Smartphones and tablets are the best known examples. As we have already seen, this market is currently dominated by Google’s Android and Apple’s iOS. Most of these devices boast multicore CPUs, GPS, cameras and other sensors, copious amounts of memory, and sophisticated operating systems. Moreover, all of them have more third-party applications (apps) than you can shake a (USB) stick at. Google has over 3 million Android apps in the Play Store and Apple has over 2 million in the App Store.
*** The Internet of Things and Embedded OS
- The *IOT(Internet of Things)* comprises all the billions of physical objects with sensors and actuators that are increasingly connected to the network, such as fridges, thermostats, security camera's motion sensor, and so on. All of these devices contain small computers and most of them run small operating system.
*** Real-Time OS
- Real-time systems are characterized as having time as a key parameter. For example, in industrial process-control systems, real-time computers have to collect data about the production process and use it to control machines in the factory. Often there are hard deadlines that must be met. For example, if a car is moving down an assembly line, certain actions must take place at certain instants of time. If, for example, a welding robot welds too early or too late, the car will be ruined. If the action absolutely must occur at a certain moment (or within a certain range), we have a *hard real-time system*. Many of these are found in industrial process control, avionics, military, and similar application areas. These systems must provide absolute guarantees that a certain action will occur by a certain time.
- *A soft real-time system* is one where missing an occasional deadline, while not desirable, is acceptable and does not cause any permanent damage. Digital audio or multimedia systems fall in this category. Smartphones are also soft real-time systems.
- We should emphasize that the categories of IoT, embedded, real-time and even handheld systems overlap considerably. Many of them have at least some soft real-time aspects. The embedded and real-time systems run only software put in by the system designers; users cannot add their own software, which makes protection easier.
*** Smart Card OS
- The smallest OS run on smart cards, which are credit-card-sized devices containing a CPU. They have very severe processing power and memory constraints. Some are powered by contacts in the reader into which they are inserted, while contactless smart cards are inductively powered (which greatly limits what they can do.) Some of them can handle only a single function, such as electronic payments, but others can handle multiple functions. Often these are proprietary systems.
- Some smart cards are Java oriented. This means that the ROM on the smart card holds an interpreter for the Java Virtual Machine(JVM). Java applets (small programs) are downloaded to the card and are interpreted by the JVM interpreter. Some of these cards can handle multiple Java applets at the same time, leading to multiprogramming and the need to schedule them. Resource management and protection also become an issue when two or more applets are present at the same time. These issues must be handled by the (usually extremely primitive) operating system present on the card.
- Many embedded systems have no protection hardware and run just a single program. That works because the system designers have total control over all the software.
** OS Concepts
*** Processes
- A process is basically a program in execution. Associated with each process is its *address space*, a list of memory locations from 0 to some maximum, which the process can read and write. The address space contains the executable program, the program's data, and its stack. Also associated with each process is a set of resources, commonly including registers(including the program counter and stack pointer), a list of open files, outstanding alarms, lists of related processes, and all the other information needed to run the program. A process is fundamentally a container that holds all the information needed to run a program.
- When a process is suspended temporarily(didn't finish executing), it must later be restarted in exactly the same state it had when it was stopped. This means that all information about the process must be explicitly saved somewhere during the suspension. In many OSs, all the information about each process, other than the contents of its own address space, is stored in an operating system table called the *process table*, which is an array of structures, one for each process currently in existence. Thus, a (suspended) process consists of its address space, usually called the *core image*, and its process table entry, which contains the contents of its registers and many other items needed to restart the process later.
- The key process-management system calls are those dealing with the creation and termination of processes. Consider a typical example. A process called the *command interpreter or (i.e., shell)* reads commands from a terminal. The user has just typed a command requesting that a program be compiled. The shell must now create a new process that will run the compiler. When that process has finished the compilation, it executes a system call to terminate itself.
- Related processes that are cooperating to get some job done often need to communicate with one another and synchronize their activities. This communication is called *interprocess communication*
- Each person authorized to use a system is assigned a *UID(User Identification)* by the system administrator. Each process started has the UID of the person who started it. On UNIX, a child process has the same UID as its parent. Users can be members of groups, each of which has a *GID(Group Identification)*.
- One UID, called the *superuser* or *root* (in UNIX), or *Administrator* (in Windows), has a special power and may override many of the protection rules.
*** Address Space
- More sophisticated OSs allows multiple programs to be in memory at the same time. To keep them from interfering with one another(and with the OS), some kind of protection mechanism is needed. While the hardware must provide this mechanism, it is the operating system that controls it.
- The above viewpoint is concerned with managing and protecting the main memory. A different, but equally important, memory-related issue is managing the address space of the processes. Normally, each process has some set of addresses it can use, typically running from 0 up to some maximum. In the simplest case, the maximum amount of address space a process has is less than the main memory. In this way, a process can fill up its address space and there will be enough room in main memory to hold it all.
- However, on many computers addresses are 32 or 64 bits, giving an address space of 2^{32} or 2^{64} bytes, respectively.
- What happens if a process has more address space than the computer has main memory and the process want to use it all? In the first computers, such a process was just out of luck. Nowadays, a technique called *virtual memory* exists, in which the OS keeps part of the address space in main memory and part on SSD or disk and shutles pieces back and forth between them as needed.
- In essence, the operating system creates the abstraction of an address space as the set of addresses a process may reference. The address space is decoupled from the machine’s physical memory and may be either larger or smaller than the physical memory. Management of address spaces and physical memory forms an important part of what an operating system does.
*** Files
- A major function of the OS is to hide the peculiarities of the SSDs, disks, and other I/O devices and present the programmer with a nice, clean abstract model of device independent files. System calls are obviously needed to create files, remove files, read files, and write files. Before a file can be read, it must be located on the storage device and opened, and after being read it should be closed, so calls are provided to do these things.
- To provide a place to keep files, most PC OSs have the concept of *directory*, sometimes called *folder* or *map*, as a way of grouping files together.
** System Calls
*** What are System Calls
- OS has two main functions: providing abstractions to user programs and managing computer's resource. Resource management is transparent to the users and are done automatically. Thus, the interface between user programs and the OS is primarily about dealing with the abstractions. To really understand what OS do, we must examine this interface closely.
- Since the actual mechanics of issuing a system call are highly machine dependent and often must be expressed in assembly code, a procedure library is provided to make it possible to make system calls from C programs and often from other languages as well.
- If a process is running a user program in user mode and needs a system service, such as reading data from file, it has to execute a *trap instruction* to transfer control to the operating system. The operating system then figures out what the calling process wants by inspecting the parameter. Then it carries out the system call and returns control to the instruction following the system call.
- The trap instruction is actually fairly similar to the procedure-call instruction in the sense that the instruction following it is taken from distant location and the return address is saved on the stack for use later.
  Nevertheless, the trap instruction also differs from the procedure-call instruction in two fundamental ways. First, as side effect, it switches into kernel mode. The procedure call instruction does not change the mode. Second, rather than giving a relative or absolute address where the procedure is located, the trap instruction can not jump to an arbitrary address. Depending on the architecture, either it jumps to a single fixed location or there is an 8-bit field in the instruction giving the index into a table in memory containing jump addresses, or equivalent.
- Making a system call is like making a special kind of procedure call -- only system calls enter the kernel mode and procedure calls do not.
- Programs should always check the results of a system call to see if an error occurred.
- System calls are performed in a series of steps. To make this concept clearer, let us examine the /read/ system call from UNIX. To make the /read/ system call, the calling program first prepares the parameters, for instance by storing them in a set of registers that by convention are used for parameters. For instance, on x86-64 CPUs, Linux, FreeBSD, Solaris and macOS use the System V AMD64 ABI *calling convention*, which means that the first six parameters are passed in registers RDI, RSI, RDX, RCX, R8, and R9. If there are more than six arguments, the remainder will be pushed onto the stack.
- POSIX has about 100 procedure calls. Some of the most important ones, grouped for convenience in four categories as follow:
  
  *Process Management*
  
  | call                              | Description                                     |
  |-----------------------------------+-------------------------------------------------|
  | pid = fork()                      | creates a child process identical to the parent |
  |-----------------------------------+-------------------------------------------------|
  | pid=waitpid(pid,&statloc,options) | wait for a child to terminate                   |
  |-----------------------------------+-------------------------------------------------|
  | s = execve(nave,argv,environp)    | Replaces process' core image                    |
  |-----------------------------------+-------------------------------------------------|
  | exit(status)                      | Terminate process execution and return status   |
  |-----------------------------------+-------------------------------------------------|

  *File Management*
  
  | call                               | Description                              |
  |------------------------------------+------------------------------------------|
  | fd = open(file, how, ...)          | Open a file for reading, writing or both |
  |------------------------------------+------------------------------------------|
  | s = close(fd)                      | close an open file                       |
  |------------------------------------+------------------------------------------|
  | n = read(fd, buffer, nbytes)       | Read data from a file into a buffer      |
  |------------------------------------+------------------------------------------|
  | n = write(fd, buffer, nbytes)      | Write data from a buffer into a file     |
  |------------------------------------+------------------------------------------|
  | position=lseek(fd, offset, whence) | Move the file pointer                    |
  |------------------------------------+------------------------------------------|
  | s = stat(name, &buf)               | Get a file's status information          |
  |------------------------------------+------------------------------------------|

  *Directory and File management*
  
  | call                          | Description                                |
  |-------------------------------+--------------------------------------------|
  | s = mkdir(name, mode)         | Create a new directory                     |
  |-------------------------------+--------------------------------------------|
  | s = rmdir(name)               | Remove an empty directory                  |
  |-------------------------------+--------------------------------------------|
  | s = link(name1, name2)        | Create new entry, name2, pointing to name1 |
  |-------------------------------+--------------------------------------------|
  | s = unlink(name)              | Remove a directory entry                   |
  |-------------------------------+--------------------------------------------|
  | s = mount(special,name, flag) | Mount a file system                        |
  |-------------------------------+--------------------------------------------|
  | s = umount(special)           | Unmount a file system                      |
  |-------------------------------+--------------------------------------------|

  *Miscellaneous*
  
  | call                     | Description                             |
  |--------------------------+-----------------------------------------|
  | s = chdir(dirname)       | change the working directory            |
  |--------------------------+-----------------------------------------|
  | s = chmode(name, mode)   | Change a file's protection bits         |
  |--------------------------+-----------------------------------------|
  | s = kill(pid, signal)    | send a signal to a process              |
  |--------------------------+-----------------------------------------|
  | seconds = time(&seconds) | Get the elapsed time since Jan. 1, 1970 |
  |--------------------------+-----------------------------------------|

*** System Calls for Process Management
- /fork/ is the only way to create a new process in POSIX. It creates an exact duplicate of the original process, including all the file descriptors, registers, -- everything. After the /fork/, the original process and the copy(parent and child) got their separate ways. All the variables have identical values at the time of the fork, but since the parent’s data are copied to create the child, subsequent changes in one of them do not affect the other one. In fact, the memory of the child may be shared *copy-on-write* with the parent. This means that parent and child share a single physical copy of the memory until one of the two modifies a value at a location in memory -- in which case the OS makes a copy of the small chunk of memory containing that location. Doing so minimizes the amount of memory that needs to be copied a priori, as much can remain shared. Moreover, part of the memory, for instance, the program text does not change at, so it can always be shared between parent and child. The /fork/ call returns a value, which is zero in the child and equals to the child's *PID(Process IDentifier)* in parent. Using the returned PID, the two processes can see which one is the parent process and which one is the child process.
- In most cases, after /fork/, the child will need to execute different code from the parent.
*** System Calls for File Management
- To read or write a file, it must first be opened. This call specifies the filename to be opened, either as an absolute path name or relative to the working directory, as well as the code for opening for reading, writing or both. The file can be closed by /close/, which makes the file descriptor available for reuse on a subsequent /open/.
- Associated with each file is a pointer that indicates the current position in the file. When reading(writing) sequentially, it normally points to the next byte to be read(written). The /lseek/ call changes the value of the position pointer, so that subsequent calls to read or write can begin anywhere in the file.
- Lseek has three parameters: the first is the file descriptor for the file, the second is a file position, and the third tells whether the file position is relative to the beginning of the file, the current position, or the end of the file. The value returned by lseek is the absolute position in the file (in bytes) after changing the pointer.
- For each file, UNIX keeps track of the file mode(regular file, special file, directory, and so on), size, time of last modification, and other information. Programs can ask to see this information via the /stat/ system call. The firs
*** System Calls for Directory Management
- The first two calls, /mkdir/ and /rmdir/ create and remove directories. The next call is /link/. Its purpose is to allow the same file to appear under two or more names, often in different directories. A typical use is to allow several members of the same programming team to share a common file, with each of them having the file appear in his own directory, possibly under different names. Having a shared file means that changes that any member of the team makes are instantly visible to other members -- there is only one file.
- Every file in UNIX has a unique number, its *i-number*, that identifies it. This i-number is an index into a table of *i-nodes*, one per file, telling who owns the file, where its disk block are, and so on. A directory is simply a file containing a set of(i-number, ASCII name) pairs.
- The /mount/ call makes it possible to integrate removable media into a single integrated file hierarchy, without having to worry about which device a file is on.
** Operating System Structure
*** Monolithic Systems
- By far the most common organization, the monolithic approach is to run the entire operating system as a single program in kernel mode. The OS is written as a collection of procedures, linked together into a single large executable binary program.
- When this technique is used, each procedure in the system is free to call any other one, if the later provides some useful computation that the former needs.
- Being able to call any procedure you want is very efficient, but having thousands of procedures that can call each other without restriction may also lead to a system that is unwieldy and difficult to understand. Also, a crash in any of these procedures will take down the entire operating system.
- To construct the actual object program of the OS when this approach is used, one must first compile all the individual procedures and then bind them all together into a single executable file using the system linker.
- In terms of information hiding, there is essentially *none* -- every procedure is to every other procedure.
- Even in monolithic systems, however, it is possible to have some structure. The services (system calls) provided by the operating system are requested by putting the parameters in a well-defined place (e.g., on the stack) and then executing a trap instruction. This instruction switches the machine from user mode to kernel mode and transfer control to the operating system.
- This organization suggests a basic structure for the operating system:
  1) A main program that invokes the requested service procedure.
  2) A set of service procedures that carry out the system calls.
  3) A set of utility procedures that help the service procedures.
- In this model, for each system call there is one service procedure that takes cares of it and executes it. The utility procedures do things that are needed by several service procedures, such as fetching data from user programs.
  See below figure:
  [[./Monolithic-System.jpeg]]
- In addition to the core operating system that is loaded when the computer is booted, many operating systems support loadable extensions, such as I/O device drivers and file systems. These components are loaded in demand. In UNIX they are called *shared libraries*. In Windows they are called *DLLs(Dynamic Link Libraries)*. They have file extension /.dll/
*** Layered Systems
*** Microkernels
- The basic idea behind the microkernel design is to achieve high reliability by splitting the operating system up into small, well-defined modules, only one of which --the microkernel-- runs in kernel mode and the rest run as relatively powerless ordinary user processes.
- In particular, by running each device driver and file system as a separate user process, a bug in one of these can crash that component, but cannot crash the entire system. Thus, a bug in the audio driver will cause the sound to be garbled or stop, but will not crash the computer. In contrast, in a monolithic system with all the drivers in the kernel, a buggy audio driver can easily reference an invalid memory address and bring the system to a grinding halt instantly.
- Many microkernels have been implemented and deployed for decades.
- With the exception of macOS, which is based on the Mach microkernel, common desktop operating system do not use microkernels. However, they are dominant in real-time, industrial, avionics, and military applications that are mission critical and have very high reliability requirements. A few of the better-known microkernels include Integrity, K42, L4, PikeOS, QNX, Symbian, and MINIX 3.
- MINIX3 has taken the idea of modularity to the limit, breaking most of the operating system up into an number of independent user-mode processes. MINIX 3 is a POSIX conformant, open source OS.
- Intel adopted MINIX 3 for its management engine in virtually all its CPU.
 *OVERVIEW OF MINIX 3*
- The MINIX 3 microkernel is only about 15,000 lines of C and some 1,400 lines of assembler for every low-level functions such as catching interrupt and switching process.
  The C code manages and schedules processes, handles interprocess communication (by passing messages between processes), and offers a set of about 40 kernel calls to allow the rest of the operating system to do its work. These calls perform functions like hooking handlers to interrupts, moving data between address spaces, and installing memory maps for new processes.(see MINIX 3 textbook by the creator for more)
*** Client-Server Model
- A slight variation of the microkernel idea is to distinguish two classes of processes, the *servers*, each of which provides some services, and the *client*, which use these services. The essence is the presence of client processes and server processes.
- Communication between clients and servers is often by message passing. To obtain a service, a client process constructs a message saying what it wants and sends it to the appropriate service. The service then does the work and sends back the answer. If the client and server happen to run on the same machine, certain optimizations are possible, but conceptually, we are still talking about message passing here.
- Since clients communicate with servers by sending messages, the client need not know whether the messages are handled locally on their own machines, or whether they are sent across a network to servers on remote machine. As far as the client is concerned, the same thing happens in both cases: request are sent and replies come back. Thus, the client-server model is an abstraction that can be used for a single machine or a network of machines.
*** Virtual Machines
- Many companies have traditionally run their mail servers, Web servers, FTP servers, and other servers on separate computers, sometimes with different operating systems. They see virtualization as a way to run them all on the same machine without having a crash of one server bring down the rest
- Virtualization is also popular in the web hosting world.
- Another use of virtualization is for end users who want to be able to run two or more operating systems at the same time.
- *Hypervisor*, also known as *Virtual Machine Monitor(VMM)*, is a software layer that allows multiple operating systems to run concurrently on a single physical machine. There are two types of hypervisors:
  1) Type 1 hypervisor: installed directly on the physical hardware. Often used in data centers and enterprise environments for server virtualization due to their performance and efficiency.
     e.g., VMware, ESXi, Microsoft Hyper-v, and xen
  2) Type 2 hypervisor: Installed on top of the conventional operating system(host OS). Typically used for personal use, development and testing environment where ease of use and flexibility are prioritized.
     e.g., VMware Workstation, Oracle VirtualBox
- In order to run virtual machine software on a computer, its CPU must be virtualizable.
- *Containers*: beside full virtualization, we can also run multiple instances of an operating system on a single machine at the same time by having the operating system itself support different systems, or *containers*. Containers are provided by the host operating system like Windows or Linux and mostly run just the user mode portion of the operating system. Each container shares the host operating system kernel and typically the binaries and libraries in a read-only fashion. This way, a Linux host can support many Linux containers. Since a container does not contain a full operating system, it can be extremely lightweight.
  Of course there are downsides to containers also.
  1) It is not possible to run a container with a completely different OS from that of the host
  2) There is no strict resource partitioning.
  3) Containers are process-level isolated. This means that a container that messes with the stability of the underlying kernel will also affect other containers.
*** Exokernels and Unikernels
- Rather than cloning the actual machine, as done with virtual machines, another strategy is partitioning it, in other words, giving each other a subset of the resources. Thus, one virtual machine may get disk block 0 to 1023, the next one may get blocks of 1024 to 2047, and so on.
- At the bottom layer, running in kernel mode, is a program called the *exokernel*. Its job is to allocate resources to virtual machines and then check attempts to use them to make sure no machine is trying to use somebody else's resources. Each user-level virtual machine can run its own Operating system, except that each one is restricted to using only the resources it has asked for and been allocated.
- The advantage of the exokernel schema is that it saves a layer of mapping. In the other design, each virtual machine thinks it has its own disk or SSD, with block running from 0 to some maximum, so the virtual machine monitor must maintain tables to remap disk block addresses(and all other resources). With the exokernel the remapping is not needed. The exokernel need only keep track of which virtual machine has been assigned which resource. This method still has the advantage of separating the multiprogramming (in the exokernel) from the user operating system code (in user space), but with less overhead, since all the exokernel has to do is keep the virtual machines out of each other’s hair.
* process and Threads
