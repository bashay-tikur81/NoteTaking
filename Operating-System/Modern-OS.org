#+TITLE: Modern Operating System
#+BOOK: Tanenbaum A, Bos H 5th Edition
#+AUTHOR: Ertale81

* Introduction
** What is an OS?
*** General Overview
- Smartphones and tablets (like the Apple iPad) are just computers in a smaller package with a touch screen. They all have operating systems. Android smartphones and tablets all run Linux as OS on the bare hardware. Android is simply a layer of software running on top of Linux.
- The program that users interact with, usually called *shell* when it is text based and the *GUI(Graphical User Interface)* when it uses icons, is actually not part of the operating system, although it uses OS to get its work done.
- Most computers have two modes of operation:
  1) *Kernel mode(supervisor mode)*- OS runs in this mode for at least some of its functionality. In this mode, it has complete access to all the hardware and can execute any instruction the machine is capable of executing.
  2) *User mode*- The rest of software runs in user mode, in which only a subset of the machine instruction is available. In particular, those instructions that affect control of the machine, determine the security boundaries, or do *I/O* are forbidden to user-mode programs.
- The user interface program, shell or GUI, is the lowest level of user mode software, and allows the user to start other programs, such as web browser, or music player ... These programs make heavy use of the operating system.
- The placement of OS is above hardware. It runs on the bare hardware and provides the base for all the other software.
*** OS as an Extended Machine
- Operating systems contain many drivers for controlling I/O devices.
- The job of OS is to create good abstractions and then implement and manage the abstract objects thus created.
- One of the major tasks of OS is to hide the (complicated, difficult, awkward...) hardware and present programs with nice, clean, elegant, consistent, abstractions to work with. OS turns the awful in to beautiful.
- OS's real customers are application programs.
*** OS as Resource Manager
- The concept of an OS as primarily providing abstractions to application programs is a top-down view. An alternative, bottom-up, view holds that the OS is there to manage all the pieces of the complex system.
- In the bottom-up view, the job of the OS is to provide for an orderly and controlled allocation of the processors, memories, and I/O devices among the various programs wanting them.
- Resource management includes *multiplexing(sharing)* resources in two different ways:
  1) *Time multiplexing-* When a resource is time multiplexed, different programs or users take turns using it. First one of them gets to use the resource, then the another, and so on. For example: with only one CPU and multiple programs that want to run on it, the OS first allocates the CPU to one program, then, after it has run long enough, another program gets to use the CPU, then another, and eventually the first one again. Determining who gets next and for how long is the task of OS. Another example of time multiplexing is sharing the printer.
  2) *Space multiplexing*- In this resource sharing, each customer gets part of the resource, instead of taking turns. For example,  main memory is normally divided up among several running programs, each one can be resident at the same time(for example, in order to take turns using the CPU). Other resources that are space multiplexed are disks and Flash drives.
** History Of OS
*** 1st Generation(1945-1955): Vacuum Tubes
- OS have historically been closely tied to the architecture of the computers on which they run.
- All programming was done in an absolute machine language, or even worst yet, by wiring up electrical circuits by connecting thousands of cables to plugboards to control the machines basic functions.
- Programming language was unknown(even assembly language was unknown). OS was unheard of.
- The usual mode of operation was for the programmer to sign up for a block of time using the sign up sheet on the wall, then come down to the machine room, insert his/her plugboard into the computer, and spend the next few hours hoping that none of the 2,000 or so vacuum tubes would burn out during the run. Virtually all the problems were simple mathematical and numerical calculations, such as grinding out tables of sines, cosines, and logarithms, or computing artillery trajectories.
- By the early 1950, the routine had improved somewhat with the introduction punched cards. It was now possible to write programs on cards and read them instead of using plugboards.
*** 2nd Generation(1955-1965): Transistors and Batch Systems
- For the first time, there was a clear separation between designers, builders, operators, programmers, and maintenance personnel.
- To run a *job* (i.e., a program or set of programs), a programmer would first write the program on paper(in FORTRAN or assembler), then punch it on cards. The programmer would then bring the card deck down to the input room, hand it to one of the operators, and go drink coffee until the output was ready.
- When the computer finished whatever job it was currently running, an operator
  would go over to the printer and tear off the output and carry it over to the output
  room, so that the programmer could collect it later. Then the operator would take one of the card decks that had been brought from the input room and read it in. If the FORTRAN compiler was needed, the operator would have to get it from a file cabinet and read it in. Much computer time was wasted while operators were walking around the machine room.
- The solution generally adopted was the *batch system*. The idea behind it was to collect a tray full of jobs in the input room and then read them onto a magnetic tape using a (inexpensive) small computers like IBM 1401, which was good at reading cards, copying tapes, and printing output, but not quite good at numerical calculations. Other, much more expensive machines, such as IBM 7094, were used for real computing.
- After about an hour of collecting a batch of jobs, the cards were read onto a magnetic tape, which was carried into the machine room.

- Batch operating system group's jobs that perform similar types of functions. These groups are called as batch and are executed at the same time. In early 1950's batch operating system got into action at that time computer was not that developed, they had less processing power and a very minimal memory. So, at that time only one job would get executed at a time. We had to wait until the job gets executed to run the next job. We are saying execution of a job not execution of a program. So what is a job?
  + The operating system/computer at that time was not interactive. What do we mean by interactive? Let's see with an ATM. If you go to an ATM, the ATM machine will interact with you by first asking your PIN. After you enter the PIN, it will ask you for amount and other services. Basically it will take information, do the processing, ask you next question or input and do processing on it. But the computers in early stages weren't interactive. They would require everything all at once.
  + So as we need to input all information at once it was given input as a job. A job would basically consist of programs, input data, and control instructions. So, how would you give this job as input to the operating system? The input device at that time used was punch card readers and punch cards were storage device. Punch cards were stiff papers which could store digital data represented by presence or absence of holes in predefined positions. Punch cards were punched by hand or machine.
  + After preparing the job, users would give it the operator. Operator was a person who would operate the computer. He would collect the jobs from the user. There was no direct interaction of user and operating system or computer. The processing and memory power of the computer back then were not so good. It was tough to execute different types of jobs each time.
  + Let's say we have three jobs. Two of them are same(job 1 and job 3) and one(job 2) is different. And you executed in job 1 \to job 2 \to job 3 sequence. The system would then reload all the resources for job 1, take input, do process and give an output. Then it will take the job 2. As the this job is different it has to deallocate the resources from previous job and allocate the resources for new job and the whole process will work. Then job 3 will be executed, again since it is difference from the previous job, deallocation of resources and allocation of resources for this job will be done.
  + Due to less memory and processing power, execution time taken was to much and deallocation and allocation of resources for each new job would take much time. *Batch processing* was the solution for this problem. The similar kinds of job were combined in batches. Using this technique system would allocate resources for a particular type of jobs which are having similar requirement will execute one by one. This will save time for making system ready for different job each time.
  + Then the operator would make collection of jobs having similar requirements in to batches and the batch was given input to the system where once the loading of resources was done. And batch was executed and the output was generated.
*** 3rd Generation (1965-1980): ICs and Multiprogramming
- By the early 1960s, most computer manufactures had two distinct incompatible product lines. On the one hand, there were the word-oriented, large-scale scientific computers, such as 7094, which were used for industrial-strength numerical calculations in science and engineering. On the other hand, there were the character-oriented, commercial computers such as 1401, which were widely used for tape sorting and printing by banks and insurance companies.
- IBM attempted to solve both these problems at a single stroke by introducing the System/360.
- The IBM 360 was the first major computer line to use IC(Integrated Circuit), thus providing a major price/performance advantage over the second generation machines, which were built up from individual transistors.
- They popularized techniques absent on second generation OS. The most important of these was *multiprogramming*. On the 7094, when the current job paused to wait for a tape or other I/O operations to complete, the CPU simply sat idle until the I/O finished. With heavily CPU-bound scientific calculations, I/O is infrequent, so this wasted time is not significant. With commercial data processing, the I/O wait time can often be 80% or 90% of the total time, so something has to be done to avoid having the(expensive) CPU be idle so much. The solution that evolved was to partition memory into several  pieces, with a different job in each partition. While one job was waiting for I/O to complete, another job could be using the CPU. If enough jobs could be held in main memory at once, the CPU could be kept busy nearly 100% of the time. Having multiple jobs safely in memory at once requires special hardware to protect each job against snooping and mischief by other ones, but the 360 and other third-generation systems were equipped with this hardware.
- Another major feature present in third-generation OS was the ability to read jobs from cards onto the disk as soon they were brought to the computer room. Then, whenever a running job finished, the operating system could load a new job from the disk into the now-empty partition and run it. This ability is called *spooling* (from *Simultaneous peripheral Operation on Line*) and was also used for output. With spooling, the 1401s were no longer needed, and much carrying of tapes disappeared.
- Third generation computers were basically batch systems.
- Another major development during the third generation was the phenomenal growth of minicomputers, starting with DEC PDP-1 in 1961. The PDP-1 had only 4K of 18-bit words, but at $120,000 per machine( less than 5% of the price of 7094). It was quickly followed by a series of other PDPs culminating(culminate- reach climactic stage) in the PDP-11.
- One of the computer scientists at Bell Labs who had worked on the MULTICS project, Ken Thompson, subsequently found a small PDP-7 minicomputer that no one was using and set out to write a stripped-down, one-user version of MULTICS. This work later developed into the *UNIX* operating system.
- Because the source code of UNIX was widely available, various organizations developed their own(incompatible versions), which lead to chaos. Two major versions developed, *System V*, from AT&T, and *BSD(Berkeley Software Distribution)* from the university of California at Berkeley. To make it possible to write program that could run on any UNIX system, IEEE developed a standard for UNIX, called *POSIX(Portable Operating System Interface)*, that most versions of UNIX now support. POSIX defines a minimal system-call interface that conformant(conformant- conforming to a particular specification or standard) UNIX system must support.
*** 4the Generation(1980-present): Personal Computers
- With the development of *LSI(Large Scale Integration)* circuits -- chips containing thousands of transistors on a square centimeter of silicon -- the age of personal computer dawned. In terms of architecture, personal computers(initially called *minicomputers*) were not all that different from minicomputers of the PDP-11 class, but in terms of price they certainly was different.
* process and Threads
